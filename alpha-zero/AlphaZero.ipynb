{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import subprocess as sub\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 5, 4, 2]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_minimum_moves(state):\n",
    "  command = [\"./solver.exe\", \"get-min\", str(len(state)), *[str(x) for x in state]]\n",
    "  run_process = sub.run(command, stdout=sub.PIPE)\n",
    "  output = int(run_process.stdout.decode())\n",
    "  return output\n",
    "\n",
    "def get_index_of_state(state):\n",
    "  command = [\"./solver.exe\", \"get-index\", str(len(state)), *[str(x) for x in state]]\n",
    "  run_process = sub.run(command, stdout=sub.PIPE)\n",
    "  output = int(run_process.stdout.decode())\n",
    "  return output\n",
    "\n",
    "def get_state_from_index(index, n):\n",
    "  command = [\"./solver.exe\", \"get-permutation\", str(n), str(index)]\n",
    "  run_process = sub.run(command, stdout=sub.PIPE)\n",
    "  output = run_process.stdout.decode().split()\n",
    "  output = list(map(int, output))\n",
    "  return output\n",
    "\n",
    "def get_random_state(n):\n",
    "  index = np.random.randint(0, math.factorial(n))\n",
    "  state = get_state_from_index(index, n)\n",
    "  return state\n",
    "\n",
    "\n",
    "class ShortSwap:\n",
    "  def __init__(self, state_size):\n",
    "    self.state_size = state_size\n",
    "    self.action_size = 2*state_size-3 # 2 moves per position except for last 2 positions\n",
    "\n",
    "  def get_position_and_move(self, action):\n",
    "    position = action%(self.state_size-1)\n",
    "    move = action//(self.state_size-1)\n",
    "    return position, move+1\n",
    "\n",
    "  def get_next_state(self, state, action):\n",
    "    position, move = self.get_position_and_move(action)\n",
    "    state = state.copy()\n",
    "    state[position], state[position+move] = state[position+move], state[position]\n",
    "    return state\n",
    "  \n",
    "  def check_win(self, state):\n",
    "    return get_index_of_state(state) == 0\n",
    "  \n",
    "  def get_value_and_terminated(self, state, moves, visited):\n",
    "    if self.check_win(state):\n",
    "      return 1+1/(moves+1), True\n",
    "    \n",
    "    if sum(self.get_valid_moves(state, visited)) == 0:\n",
    "      return -1, True\n",
    "    return 0, False\n",
    "  \n",
    "  def get_valid_moves(self, state, visited):\n",
    "    state = state.copy()\n",
    "    valid_moves = np.ones(self.action_size)\n",
    "    for position in range(self.state_size-1):\n",
    "      for move in range(1,2+1):\n",
    "        action = (move-1)*(self.state_size-1) + position\n",
    "        if action >= self.action_size:\n",
    "          continue\n",
    "\n",
    "        state[position], state[position+move] = state[position+move], state[position]\n",
    "        if get_index_of_state(state) in visited:\n",
    "          valid_moves[action] = 0\n",
    "        state[position], state[position+move] = state[position+move], state[position]\n",
    "    return valid_moves\n",
    "\n",
    "  def get_encoded_state(self, state):\n",
    "    encoded_state = np.array(state).astype(np.float32).reshape(1, -1)\n",
    "    return encoded_state\n",
    "\n",
    "  def get_initial_state(self):\n",
    "    return get_random_state(self.state_size)\n",
    "\n",
    "n = 5\n",
    "ss = ShortSwap(n)\n",
    "get_random_state(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "  def __init__(self, game, num_resBlocks, num_hidden, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.device = device\n",
    "    self.startBlock = nn.Sequential(\n",
    "      nn.Conv1d(1, num_hidden, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm1d(num_hidden),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.backBone = nn.ModuleList(\n",
    "      [ResBlock(num_hidden) for i in range(num_resBlocks)]\n",
    "    )\n",
    "\n",
    "    self.policyHead = nn.Sequential(\n",
    "      nn.Conv1d(num_hidden, 32, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm1d(32),\n",
    "      nn.ReLU(),\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(32*game.state_size, game.action_size)\n",
    "    )    \n",
    "\n",
    "    self.valueHead = nn.Sequential(\n",
    "      nn.Conv1d(num_hidden, 3, kernel_size=3, padding=1),\n",
    "      nn.BatchNorm1d(3),\n",
    "      nn.ReLU(),\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(3*game.state_size, 1),\n",
    "      nn.Tanh()\n",
    "    )\n",
    "\n",
    "    self.to(device)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = self.startBlock(x)\n",
    "    for resBlock in self.backBone:\n",
    "      x = resBlock(x)\n",
    "    policy = self.policyHead(x)\n",
    "    value = self.valueHead(x)\n",
    "\n",
    "    return policy, value\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "  def __init__(self, num_hidden):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv1d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "    self.bn1 = nn.BatchNorm1d(num_hidden)\n",
    "    self.conv2 = nn.Conv1d(num_hidden, num_hidden, kernel_size=3, padding=1)\n",
    "    self.bn2 = nn.BatchNorm1d(num_hidden)\n",
    "\n",
    "  def forward(self, x):\n",
    "    residual = x\n",
    "    x = F.relu(self.bn1(self.conv1(x)))\n",
    "    x = F.relu(self.bn2(self.conv2(x)) + residual)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "  def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n",
    "    self.game = game\n",
    "    self.args = args\n",
    "    self.state = state\n",
    "    self.parent = parent\n",
    "    self.action_taken = action_taken\n",
    "    self.prior = prior\n",
    "\n",
    "    if self.parent is None:\n",
    "      self.moves = 0\n",
    "    else:\n",
    "      self.moves = self.parent.moves+1\n",
    "    \n",
    "    self.children = []\n",
    "\n",
    "    self.visit_count = visit_count\n",
    "    self.value_sum = 0\n",
    "\n",
    "  def is_fully_expanded(self):\n",
    "    return len(self.children) > 0\n",
    "  \n",
    "  def select(self):\n",
    "    best_child = None\n",
    "    best_ucb = -np.inf\n",
    "    for child in self.children:\n",
    "      ucb = self.get_ucb(child)\n",
    "      if ucb > best_ucb:\n",
    "        best_ucb = ucb\n",
    "        best_child = child\n",
    "\n",
    "    return best_child\n",
    "  \n",
    "  def get_ucb(self, child):\n",
    "    if child.visit_count == 0:\n",
    "      q_value = 0\n",
    "    else:\n",
    "      q_value = 1 - (child.value_sum / child.visit_count + 1)/2\n",
    "    return q_value + self.args['C'] * child.prior * math.sqrt(self.visit_count) / (1+child.visit_count)\n",
    "  \n",
    "  def expand(self, policy):\n",
    "    for action, prob in enumerate(policy):\n",
    "      if prob > 0:\n",
    "        child_state = self.state.copy()\n",
    "        child_state = self.game.get_next_state(child_state, action)\n",
    "        \n",
    "        child = Node(self.game, self.args, child_state, self, action, prob)\n",
    "        self.children.append(child)\n",
    "    \n",
    "  def backpropagate(self, value):\n",
    "    self.value_sum += value\n",
    "    self.visit_count += 1\n",
    "\n",
    "    if self.parent is not None:\n",
    "      self.parent.backpropagate(value)\n",
    "\n",
    "class MCTS:\n",
    "  def __init__(self, game, args, model):\n",
    "    self.game = game\n",
    "    self.args = args\n",
    "    self.model = model\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def search(self, state, visited):\n",
    "    root = Node(self.game, self.args, state, visit_count=1)\n",
    "\n",
    "    policy, _ = self.model(\n",
    "      torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n",
    "    )\n",
    "    policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    policy = (1-self.args[\"dirichlet_epsilon\"])*policy + self.args[\"dirichlet_epsilon\"]\\\n",
    "      *np.random.dirichlet([self.args[\"dirichlet_alpha\"]*self.game.action_size])\n",
    "    valid_moves = self.game.get_valid_moves(state, visited)\n",
    "    policy *= valid_moves\n",
    "    policy /= np.sum(policy)\n",
    "    root.expand(policy)\n",
    "\n",
    "    for search in range(self.args[\"num_searches\"]):\n",
    "      node = root\n",
    "      while node.is_fully_expanded():\n",
    "        node = node.select()\n",
    "        \n",
    "      value, is_terminal = self.game.get_value_and_terminated(node.state, node.moves, visited)\n",
    "\n",
    "      if not is_terminal:\n",
    "        policy, value = self.model(\n",
    "          torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n",
    "        )\n",
    "        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n",
    "        valid_moves = self.game.get_valid_moves(node.state, visited)\n",
    "        policy *= valid_moves\n",
    "        policy /= np.sum(policy)\n",
    "\n",
    "        value = value.item()\n",
    "\n",
    "        node.expand(policy)\n",
    "\n",
    "      node.backpropagate(value)\n",
    "\n",
    "    action_probs = np.zeros(self.game.action_size)\n",
    "    for child in root.children:\n",
    "      action_probs[child.action_taken] = child.visit_count\n",
    "    action_probs /= np.sum(action_probs)\n",
    "\n",
    "    return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZero:\n",
    "  def __init__(self, model, optimizer, game, args):\n",
    "    self.model = model\n",
    "    self.optimizer = optimizer\n",
    "    self.game = game\n",
    "    self.args = args\n",
    "    self.mcts = MCTS(game, args, model)\n",
    "\n",
    "  def selfPlay(self):\n",
    "    memory = []\n",
    "    visited = set()\n",
    "    state = self.game.get_initial_state()\n",
    "    visited.add(get_index_of_state(state))\n",
    "    moves = 0\n",
    "    while True:\n",
    "      action_probs = self.mcts.search(state, visited)\n",
    "\n",
    "      memory.append((state, action_probs))\n",
    "\n",
    "      temperature_action_probs = action_probs ** (1 / self.args[\"temperature\"])\n",
    "      temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "      action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n",
    "      state = self.game.get_next_state(state, action)\n",
    "      visited.add(get_index_of_state(state))\n",
    "\n",
    "      value, is_terminal = self.game.get_value_and_terminated(state, moves, visited)\n",
    "      print(state)\n",
    "      moves += 1\n",
    "      if is_terminal:\n",
    "        returnMemory = []\n",
    "        for hist_neutral_state, hist_action_probs in memory:\n",
    "          hist_outcome = value\n",
    "          returnMemory.append((\n",
    "            self.game.get_encoded_state(hist_neutral_state),\n",
    "            hist_action_probs,\n",
    "            hist_outcome\n",
    "          ))\n",
    "          return returnMemory\n",
    "\n",
    "  def train(self, memory):\n",
    "    random.shuffle(memory)\n",
    "    for batchIdx in range(0, len(memory), self.args[\"batch_size\"]):\n",
    "      sample = memory[batchIdx:min(len(memory), batchIdx + self.args[\"batch_size\"])]\n",
    "      state, policy_targets, value_targets = zip(*sample)\n",
    "      state = torch.tensor(np.array(state), dtype=torch.float32, device=self.model.device)\n",
    "      policy_targets = torch.tensor(np.array(policy_targets), dtype=torch.float32, device=self.model.device)\n",
    "      value_targets = torch.tensor(np.array(value_targets).reshape(-1,1), dtype=torch.float32, device=self.model.device)\n",
    "\n",
    "      out_policy, out_value = self.model(state)\n",
    "\n",
    "      policy_loss = F.cross_entropy(out_policy, policy_targets)\n",
    "      value_loss = F.mse_loss(out_value, value_targets)\n",
    "\n",
    "      loss = policy_loss + value_loss\n",
    "\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "\n",
    "  def learn(self):\n",
    "    for iteration in range(self.args[\"num_iterations\"]):\n",
    "      memory = []\n",
    "\n",
    "      self.model.eval()\n",
    "      for selfPlay_iteration in range(self.args[\"num_selfPlay_iterations\"]):\n",
    "        print(f\"Playing game {selfPlay_iteration}...\")\n",
    "        memory += self.selfPlay()\n",
    "\n",
    "      self.model.train()\n",
    "      for epoch in range(self.args[\"num_epochs\"]):\n",
    "        self.train(memory)\n",
    "\n",
    "      torch.save(self.model.state_dict(), f\"model_{iteration}.pt\")\n",
    "      torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing game 0...\n",
      "[1, 2, 3, 4]\n",
      "Playing game 0...\n",
      "[3, 1, 2, 4]\n",
      "[3, 2, 1, 4]\n",
      "[3, 4, 1, 2]\n",
      "[1, 4, 3, 2]\n",
      "[4, 1, 3, 2]\n",
      "[4, 3, 1, 2]\n",
      "[1, 3, 4, 2]\n",
      "[1, 2, 4, 3]\n",
      "[1, 4, 2, 3]\n",
      "[2, 4, 1, 3]\n",
      "[2, 3, 1, 4]\n",
      "[2, 3, 4, 1]\n",
      "[2, 4, 3, 1]\n",
      "[2, 1, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "Playing game 0...\n",
      "[2, 3, 1, 4]\n",
      "[2, 3, 4, 1]\n",
      "[2, 4, 3, 1]\n",
      "[3, 4, 2, 1]\n",
      "[3, 4, 1, 2]\n",
      "[3, 2, 1, 4]\n",
      "[3, 2, 4, 1]\n",
      "[4, 2, 3, 1]\n",
      "[4, 2, 1, 3]\n",
      "[2, 4, 1, 3]\n",
      "[1, 4, 2, 3]\n",
      "[1, 4, 3, 2]\n",
      "[4, 1, 3, 2]\n",
      "[4, 3, 1, 2]\n",
      "[4, 3, 2, 1]\n",
      "[4, 1, 2, 3]\n",
      "[2, 1, 4, 3]\n",
      "[2, 1, 3, 4]\n",
      "[3, 1, 2, 4]\n",
      "[3, 1, 4, 2]\n",
      "[1, 3, 4, 2]\n",
      "[1, 2, 4, 3]\n",
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "N = 4\n",
    "shortswap = ShortSwap(N)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet(shortswap, 4, 64, device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "args = {\n",
    "  \"C\": 2,\n",
    "  \"num_searches\": 10,\n",
    "  \"num_iterations\": 3,\n",
    "  \"num_selfPlay_iterations\": 1,\n",
    "  \"num_epochs\": 4,\n",
    "  \"temperature\": 1.25,\n",
    "  \"dirichlet_epsilon\": 0.25,\n",
    "  \"dirichlet_alpha\": 0.3,\n",
    "  \"batch_size\": 32\n",
    "}\n",
    "\n",
    "alphaZero = AlphaZero(model, optimizer, shortswap, args)\n",
    "alphaZero.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 1, 4]\n",
      "[2, 3, 1, 4]\n",
      "[2, 3, 4, 1]\n",
      "[2, 1, 4, 3]\n",
      "[4, 1, 2, 3]\n",
      "[4, 2, 1, 3]\n",
      "[4, 2, 3, 1]\n",
      "[4, 3, 2, 1]\n",
      "[3, 4, 2, 1]\n",
      "[2, 4, 3, 1]\n",
      "[2, 1, 3, 4]\n",
      "[3, 1, 2, 4]\n",
      "[1, 3, 2, 4]\n",
      "[1, 4, 2, 3]\n",
      "[2, 4, 1, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1, 15)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def play_game(state):\n",
    "  mcts = MCTS(shortswap, args, model)\n",
    "  visited = set()\n",
    "  visited.add(get_index_of_state(state))\n",
    "  moves = 0\n",
    "  value, is_terminal = shortswap.get_value_and_terminated(state, moves, visited)\n",
    "    print(state)\n",
    "  \n",
    "  while True:\n",
    "    action_probs = mcts.search(state, visited)\n",
    "\n",
    "    temperature_action_probs = action_probs ** (1 / args[\"temperature\"])\n",
    "    temperature_action_probs /= np.sum(temperature_action_probs)\n",
    "    action = np.random.choice(shortswap.action_size, p=temperature_action_probs)\n",
    "    state = shortswap.get_next_state(state, action)\n",
    "    visited.add(get_index_of_state(state))\n",
    "\n",
    "    value, is_terminal = shortswap.get_value_and_terminated(state, moves, visited)\n",
    "    print(state)\n",
    "    moves += 1\n",
    "    if is_terminal:\n",
    "      return value, moves\n",
    "\n",
    "state=[1,3,4,2]\n",
    "play_game([1,2,3,4])\n",
    "get_minimum_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
